{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# Brain Tumor Segmentation with U-Net ðŸ§©\n",
        "\n",
        "**Objective**:\n",
        "Develop and evaluate a resource-efficient U-Net for delineating brain tumor subregions from multi-modal MRI.\n",
        "\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "_kl_WNgO5RaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 1. Setup ðŸ“¦\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "betb6FumJ_yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Management\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Data Visualization\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Computational Modules\n",
        "import math\n",
        "from scipy.stats import norm, zscore\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "\n",
        "# Deep Learning Frameworks\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# Utils\n",
        "from collections import defaultdict\n",
        "import h5py\n",
        "from google.colab import drive\n",
        "from IPython.display import display, HTML, IFrame, Image\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('mode.chained_assignment', None)\n",
        "sns.set_style('darkgrid', {'grid.color':'0.9','xtick.bottom':True,'ytick.left':True})\n"
      ],
      "metadata": {
        "id": "Z4L1DiE45Tk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 2. Data Access ðŸ“‚\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-q5A4-B0tL6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define project root\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8QbkH2Uh5KG",
        "outputId": "2bf9eb1b-4802-4153-d438-8c3237d1f34c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define file paths relative to drive structure\n",
        "data_path = '/content/drive/MyDrive/BraTS_Data/BraTS2020_training_data/content/data'\n",
        "meta_data_path = '/content/drive/MyDrive/BraTS_Data/BraTS2020_training_data/content/data/meta_data.csv'\n",
        "name_mapping_path = '/content/drive/MyDrive/BraTS_Data/BraTS2020_training_data/content/data/name_mapping.csv'\n",
        "survival_info_path = '/content/drive/MyDrive/BraTS_Data/BraTS2020_training_data/content/data/survival_info.csv'\n",
        "slice_paths = sorted(glob.glob(os.path.join(data_path, '*.h5')))  # get all .h5 files"
      ],
      "metadata": {
        "id": "BlMZmYFKKpm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSVs into local dataframes\n",
        "meta_data_df = pd.read_csv(meta_data_path)\n",
        "name_mapping_df = pd.read_csv(name_mapping_path)\n",
        "survival_info_df = pd.read_csv(survival_info_path)"
      ],
      "metadata": {
        "id": "WGJPAwgftX92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TO BE EXECUTED BEFORE TRAINING FOR FULL DATASET ###\n",
        "### Copy dataset locally and use instead for faster access ###\n",
        "\n",
        "# !cp -r /content/drive/MyDrive/BraTS_Data /content/\n",
        "# data_path = '/content/BraTS_Data/BraTS2020_training_data/content/data'\n",
        "# slice_paths = sorted(glob.glob(os.path.join(data_path, '*.h5')))"
      ],
      "metadata": {
        "id": "iaD5vIEuVS9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 3. Model Prerequisites âš™ï¸\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "zSyoNkEttb4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3a. Split imaging data into training and validation sets.**"
      ],
      "metadata": {
        "id": "FEN5fz-ExwwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this v1 (baseline model), only a limited set of MRI volumes will be used due to runtime trouble on free-tier Colab. Once an entire workflow is established, a more fleshed-out splitting will take place for future versions."
      ],
      "metadata": {
        "id": "C45gjdJeC40x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract random patient/volume IDs for baseline model\n",
        "random.seed(42)\n",
        "selected_patients = random.sample(range(0, 368), 10)\n",
        "print('Randomly selected patient IDs: ', selected_patients)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAFyKiC39SuP",
        "outputId": "33719f77-f1c3-417b-c9de-d8b225d4128d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Randomly selected patient IDs:  [327, 57, 12, 140, 125, 114, 71, 52, 346, 279]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert numeric IDs into corresponding filename prefixes\n",
        "selected_prefixes = [f'volume_{pid}_' for pid in selected_patients]\n",
        "selected_prefixes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1oNS6889vkx",
        "outputId": "8d117be0-af09-47ae-dcac-3ab41c15d6e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['volume_327_',\n",
              " 'volume_57_',\n",
              " 'volume_12_',\n",
              " 'volume_140_',\n",
              " 'volume_125_',\n",
              " 'volume_114_',\n",
              " 'volume_71_',\n",
              " 'volume_52_',\n",
              " 'volume_346_',\n",
              " 'volume_279_']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect all slices belonging to selected patients\n",
        "selected_slice_paths = [\n",
        "    path for path in slice_paths\n",
        "    if any(prefix in os.path.basename(path) for prefix in selected_prefixes)\n",
        "]\n",
        "print('Total selected slices: ', len(selected_slice_paths))\n",
        "# selected_slice_paths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oXsjqMb-cBk",
        "outputId": "db3ddec8-fbf4-4706-c051-b4dbbb12263b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total selected slices:  1550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy selected slices to local runtime (for faster I/O during training)\n",
        "local_data_path = '/content/baseline_data'\n",
        "os.makedirs(local_data_path, exist_ok=True)\n",
        "for path in selected_slice_paths:\n",
        "    filename = os.path.basename(path)\n",
        "    !cp '{path}' '{local_data_path}/{filename}'"
      ],
      "metadata": {
        "id": "18lKEuic-sOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild slice list from local directory\n",
        "local_slice_paths = sorted(glob.glob(os.path.join(local_data_path, '*.h5')))\n",
        "print('Local slice count: ', len(local_slice_paths))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lfDSCaB-5pR",
        "outputId": "20f401e8-f4e0-427b-b5a1-e8714ed32276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local slice count:  1550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group slices by patient (patient-level split)\n",
        "patient_to_slices = defaultdict(list)\n",
        "for path in local_slice_paths:\n",
        "    filename = os.path.basename(path)\n",
        "    patient_id = filename.split('_slice')[0]\n",
        "    patient_to_slices[patient_id].append(path)\n",
        "print(f'Patients found locally: {list(patient_to_slices.keys())}')"
      ],
      "metadata": {
        "id": "yL8GWqdXp1zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b044539c-7aea-401b-f17f-f84ca4cec554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patients found locally: ['volume_114', 'volume_125', 'volume_12', 'volume_140', 'volume_279', 'volume_327', 'volume_346', 'volume_52', 'volume_57', 'volume_71']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seperate train and validation patients\n",
        "patient_ids = list(patient_to_slices.keys())\n",
        "\n",
        "train_patients = patient_ids[:8]\n",
        "val_patients = patient_ids[8:]\n",
        "\n",
        "print('Train patients:', train_patients)\n",
        "print('Val patients:', val_patients)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRwvyvDPt1Qb",
        "outputId": "b23389b4-eb25-4120-fe11-8617e1b92943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train patients: ['volume_114', 'volume_125', 'volume_12', 'volume_140', 'volume_279', 'volume_327', 'volume_346', 'volume_52']\n",
            "Val patients: ['volume_57', 'volume_71']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate train and validation slice paths\n",
        "train_paths = []\n",
        "val_paths = []\n",
        "\n",
        "for pid in train_patients:\n",
        "    train_paths.extend(patient_to_slices[pid])\n",
        "for pid in val_patients:\n",
        "    val_paths.extend(patient_to_slices[pid])\n",
        "\n",
        "print('Train slices:', len(train_paths))\n",
        "print('Val slices:', len(val_paths))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AFCE3bIt1Sh",
        "outputId": "c833358d-58e5-430c-e27f-8bfef73d8b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train slices: 1240\n",
            "Val slices: 310\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Patient-Level Subset Selection and Split**\n",
        "\n",
        "To keep training within runtime limits, a small baseline subset was created by randomly selecting four patients (seed = 42). All slices belonging to these patients were identified and copied locally to the Colab runtime to ensure faster disk access during training.\n",
        "\n",
        "A deterministic 3:1 patient-level split was then applied. The split is performed at the patient level (not slice level) to prevent data leakage, ensuring that no slices from the same volume appear in both sets.\n",
        "\n",
        "This setup keeps the experiment lightweight while preserving proper evaluation structure."
      ],
      "metadata": {
        "id": "fjOSZdahNNer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3b. Format and standardize slices via PyTorch Dataset.**"
      ],
      "metadata": {
        "id": "UMeBiPnZIB3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BraTSDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for loading 2D BraTS slices stored as .h5 files,\n",
        "    with optional minimal augmentations for v2.\n",
        "\n",
        "    Each file contains:\n",
        "        - image: (H, W, 4)\n",
        "              MRI modalities in order: [T1, T1Gd, T2, T2-FLAIR]\n",
        "        - mask:  (H, W, 3)\n",
        "              Binary tumor subregions: [NEC/NET, ED, ET]\n",
        "\n",
        "    Output format:\n",
        "        - image tensor: (4, H, W), float32\n",
        "        - mask tensor:  (3, H, W), float32\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, slice_paths, augment=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            slice_paths (list): List of file paths to .h5 slice files.\n",
        "            augment (bool): Whether to apply on-the-fly augmentations.\n",
        "        \"\"\"\n",
        "        self.slice_paths = slice_paths\n",
        "        self.augment = augment\n",
        "\n",
        "        # Minimal v2 augmentations\n",
        "        self.transform = A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.5),\n",
        "            A.Rotate(limit=15, p=0.5),  # Â±15 degree rotations\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns total number of slices.\"\"\"\n",
        "        return len(self.slice_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Loads and processes a single slice from disk with optional augmentation.\n",
        "\n",
        "        Steps:\n",
        "            1. Load image + mask from disk (lazy loading)\n",
        "            2. Convert to float32\n",
        "            3. Apply per-slice, per-modality z-score normalization\n",
        "            4. Apply augmentation if enabled\n",
        "            5. Convert to channel-first format (C, H, W)\n",
        "            6. Convert to torch.Tensor\n",
        "        \"\"\"\n",
        "        file_path = self.slice_paths[idx]\n",
        "\n",
        "        # Load slice from disk (lazy loading)\n",
        "        with h5py.File(file_path, 'r') as file:\n",
        "            image = file['image'][:]   # (H, W, 4)\n",
        "            mask  = file['mask'][:]    # (H, W, 3)\n",
        "\n",
        "        # Convert to float32\n",
        "        image = image.astype(np.float32)\n",
        "        mask  = mask.astype(np.float32)\n",
        "\n",
        "        # Per-modality z-score normalization\n",
        "        for c in range(image.shape[-1]):\n",
        "            channel = image[:, :, c]\n",
        "            mean = channel.mean()\n",
        "            std = channel.std()\n",
        "            if std > 0:\n",
        "                image[:, :, c] = (channel - mean) / std\n",
        "            else:\n",
        "                image[:, :, c] = channel - mean\n",
        "\n",
        "        # Apply augmentation if enabled\n",
        "        if self.augment:\n",
        "            # Albumentations expects HWC, mask can have multiple channels\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask  = augmented['mask']\n",
        "\n",
        "        # Convert to channel-first format (C, H, W)\n",
        "        image = np.transpose(image, (2, 0, 1))\n",
        "        mask  = np.transpose(mask, (2, 0, 1))\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        image_tensor = torch.from_numpy(image)\n",
        "        mask_tensor  = torch.from_numpy(mask)\n",
        "\n",
        "        return image_tensor, mask_tensor"
      ],
      "metadata": {
        "id": "wXfnBtvwIK1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement processing steps onto train and validation sets\n",
        "train_dataset = BraTSDataset(train_paths, augment=True)\n",
        "val_dataset   = BraTSDataset(val_paths, augment=True)"
      ],
      "metadata": {
        "id": "5BBFdZSIIK3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect example output\n",
        "sample_img, sample_mask = train_dataset[0]\n",
        "print('Image shape:', sample_img.shape)   # (4, 240, 240)\n",
        "print('Mask shape:', sample_mask.shape)   # (3, 240, 240)\n",
        "print('Image dtype:', sample_img.dtype)\n",
        "print('Mask dtype:', sample_mask.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOviZ7ceaWeO",
        "outputId": "8a0b7b7b-7f8b-4128-b029-5aea90865059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: torch.Size([4, 240, 240])\n",
            "Mask shape: torch.Size([3, 240, 240])\n",
            "Image dtype: torch.float32\n",
            "Mask dtype: torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Implementation**:\n",
        "\n",
        "Before training, MRI slices must be properly formatted and standardized to ensure consistent numerical representation, improve training stability, and enable efficient batch loading for the U-Net.\n",
        "\n",
        "A custom PyTorch `Dataset` class is defined to load individual 2D slices stored as *.h5* files. Each file contains a 4-channel MRI image (T1, T1Gd, T2, T2-FLAIR) and a 3-channel binary segmentation mask representing tumor subregions.\n",
        "\n",
        "Data is loaded lazily within `__getitem__`, meaning slices are read from disk only when needed. This avoids loading the entire dataset into memory and keeps the pipeline scalable.\n",
        "\n",
        "Images and masks are converted to *float32* for GPU compatibility and reduced memory usage. Each MRI modality channel is also z-score normalized per slice to standardize intensities and stabilize training. This should improve model convergence and help the network learn more effectively. The arrays are then rearranged to channel-first format **(C, H, W)**, which is required by PyTorch convolutional layers. A sample output was generated to inspect it for expected format.\n",
        "\n",
        "This design separates data handling from modeling logic, making the pipeline modular, extensible, and ready for the U-Net baseline."
      ],
      "metadata": {
        "id": "l0VocrLUNuf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3c. Create DataLoaders for batching and training.**"
      ],
      "metadata": {
        "id": "tvnlCy3wV4eY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to set up data generators to produce iterable batches for training and validation."
      ],
      "metadata": {
        "id": "W35Y_l9VUChm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure batch size based on v1 constrainsts (GPU + slice size)\n",
        "batch_size = 8"
      ],
      "metadata": {
        "id": "WKReuzB6X4Ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders to handles batching, shuffling, parallel loading, and GPU memory pinning\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,            # shuffle training slices each epoch (ensures SGD sees slices in different order each epoch)\n",
        "    num_workers=2,           # parallel data loading; adjust based on Colab memory / CPU\n",
        "    pin_memory=True          # speeds up transfers to GPU\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,            # maintain deterministic order (i.e., no shuffle) for validation\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "X881JTQDU5X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_images, sample_masks = next(iter(train_loader))\n",
        "print('Batch image shape:', sample_images.shape)  # (B, 4, H, W)\n",
        "print('Batch mask shape: ', sample_masks.shape)   # (B, 3, H, W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiVjek4qZ6ep",
        "outputId": "019492d7-df07-463a-a367-4a8369aeba23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch image shape: torch.Size([8, 4, 240, 240])\n",
            "Batch mask shape:  torch.Size([8, 3, 240, 240])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DataLoader Implementation**:\n",
        "\n",
        "With the Dataset class defined, we now set up PyTorch `DataLoader` to efficiently batch and serve slices during training and validation.\n",
        "\n",
        "- **Training DataLoader**: shuffles slices each epoch to provide stochasticity for gradient descent, uses multiple workers for parallel loading, and pins memory for faster GPU transfer.\n",
        "- **Validation DataLoader**: does not shuffle to maintain deterministic evaluation order but still benefits from batching and parallel loading.\n",
        "\n",
        "Inspecting a sample batch image and mask shape confirms that each batch has the expected shape: images **(B, 4, H, W)** and masks **(B, 3, H, W)**. This ensures that data is correctly formatted, normalized, and ready for input into a segmentation model."
      ],
      "metadata": {
        "id": "4SZUAY9fU7PH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 4. Model Architecture ðŸ—ï¸\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "M7HbFiEEWzNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the data is preprocessed and loaded in batches, the next step is to define the neural network. A **U-Net** will be used as it is specifically designed for image segmentation.\n",
        "\n",
        "- U-Net takes an input image (here, 4 MRI channels) and outputs a segmentation mask (here, 3 channels for necrotic core, edema, enhancing tumor).\n",
        "- It has a **downsampling path** that captures context and an **upsampling path** that recovers spatial details.\n",
        "- The network will learn, for each pixel, whether it belongs to background or one of the tumor sub-regions.\n",
        "\n",
        "This setup allows the model to predict multi-class masks directly from MRI slices."
      ],
      "metadata": {
        "id": "YQl_XrYNXNUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4a. Integrate attention gates in the U-Net skip connections.**"
      ],
      "metadata": {
        "id": "v0MxecEWRs7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionGate(nn.Module):\n",
        "    def __init__(self, F_g, F_l, F_int):\n",
        "        \"\"\"\n",
        "        F_g: number of channels in the gating signal (decoder feature)\n",
        "        F_l: number of channels in the encoder feature (skip connection)\n",
        "        F_int: number of intermediate channels for the attention computation\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Linear transform to reduce decoder channels to intermediate dimension\n",
        "        self.W_g = nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "        # Linear transform to reduce encoder channels to intermediate dimension\n",
        "        self.W_x = nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "        # Combine the intermediate features and produce a single attention map\n",
        "        self.psi = nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "\n",
        "        # Activation functions\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        \"\"\"\n",
        "        g: gating signal from decoder (decoder feature map)\n",
        "        x: skip connection from encoder (encoder feature map)\n",
        "        \"\"\"\n",
        "        # Transform decoder and encoder features to intermediate dimension\n",
        "        g1 = self.W_g(g)\n",
        "        x1 = self.W_x(x)\n",
        "\n",
        "        # Sum and apply ReLU activation\n",
        "        psi = self.relu(g1 + x1)\n",
        "\n",
        "        # Produce attention map with values between 0 and 1\n",
        "        psi = self.sigmoid(self.psi(psi))\n",
        "\n",
        "        # Apply attention map to encoder feature map\n",
        "        # suppress irrelevant regions, emphasize important ones\n",
        "        return x * psi  # gated skip connection\n"
      ],
      "metadata": {
        "id": "CgIPbEi1RtZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plan for v2 is to add attention gates in the skip connections of U-Net. Attention gates allow the U-Net to focus on relevant regions in the encoder feature maps before concatenating with decoder features. This suppresses background and emphasizes tumor-relevant features in skip connections, improving segmentation quality. This will still be lightweight enough to keep training fast on 2D slices."
      ],
      "metadata": {
        "id": "4VjNvcHhR9w1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4b. Define a baseline U-Net for brain tumor segmentation.**"
      ],
      "metadata": {
        "id": "0v0piAkvchtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeanUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net with Attention Gates for multi-class 2D brain tumor segmentation.\n",
        "\n",
        "    Input: 4-channel MRI slice (T1, T1Gd, T2, T2-FLAIR)\n",
        "    Output: 3-channel segmentation mask (Necrotic Core, Edema, Enhancing Tumor)\n",
        "    Attention gates are applied to skip connections to emphasize relevant tumor regions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels=4, out_channels=3, init_features=16):\n",
        "        super().__init__()\n",
        "        features = init_features\n",
        "\n",
        "        ### Encoder (downsampling path)\n",
        "        self.encoder1 = self._conv_block(in_channels, features)\n",
        "        self.pool1    = nn.MaxPool2d(2)\n",
        "\n",
        "        self.encoder2 = self._conv_block(features, features*2)\n",
        "        self.pool2    = nn.MaxPool2d(2)\n",
        "\n",
        "        self.encoder3 = self._conv_block(features*2, features*4)\n",
        "        self.pool3    = nn.MaxPool2d(2)\n",
        "\n",
        "        ### Bottleneck\n",
        "        self.bottleneck = self._conv_block(features*4, features*8)\n",
        "\n",
        "        #### Decoder (upsampling path) with attention gates\n",
        "        self.upconv3 = nn.ConvTranspose2d(features*8, features*4, kernel_size=2, stride=2)\n",
        "        self.attn3   = AttentionGate(F_g=features*4, F_l=features*4, F_int=features*2)\n",
        "        self.decoder3 = self._conv_block(features*8, features*4)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(features*4, features*2, kernel_size=2, stride=2)\n",
        "        self.attn2   = AttentionGate(F_g=features*2, F_l=features*2, F_int=features)\n",
        "        self.decoder2 = self._conv_block(features*4, features*2)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(features*2, features, kernel_size=2, stride=2)\n",
        "        self.attn1   = AttentionGate(F_g=features, F_l=features, F_int=features//2)\n",
        "        self.decoder1 = self._conv_block(features*2, features)\n",
        "\n",
        "        ### Output layer\n",
        "        self.conv_final = nn.Conv2d(features, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### Encoder forward pass\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool1(enc1))\n",
        "        enc3 = self.encoder3(self.pool2(enc2))\n",
        "\n",
        "        ### Bottleneck\n",
        "        bottleneck = self.bottleneck(self.pool3(enc3))\n",
        "\n",
        "        ### Decoder forward pass with attention gates\n",
        "        dec3 = self.upconv3(bottleneck)\n",
        "        gated_enc3 = self.attn3(dec3, enc3)\n",
        "        dec3 = torch.cat((dec3, gated_enc3), dim=1)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        gated_enc2 = self.attn2(dec2, enc2)\n",
        "        dec2 = torch.cat((dec2, gated_enc2), dim=1)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        gated_enc1 = self.attn1(dec1, enc1)\n",
        "        dec1 = torch.cat((dec1, gated_enc1), dim=1)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "\n",
        "        return self.conv_final(dec1)\n",
        "\n",
        "    ### 2-conv block\n",
        "    def _conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )"
      ],
      "metadata": {
        "id": "dEJigYnxdKbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define computation mode: GPU if available, else CPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device: ', device)\n",
        "\n",
        "# Instantiate the attention-enhanced U-Net\n",
        "model = LeanUNet().to(device)\n",
        "# print(model)\n",
        "\n",
        "# Quick unit test: check output shape for a dummy batch\n",
        "ex_input = torch.randn(2, 4, 240, 240).to(device)  # batch_size=2, 4 MRI channels\n",
        "ex_output = model(ex_input)\n",
        "print('Example output shape:', ex_output.shape)  # Expected: (2, 3, 240, 240)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvJ-zKoPdNoM",
        "outputId": "e0a7aba8-e313-47d4-d6e3-59f4b3e9b8ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n",
            "Example output shape: torch.Size([2, 3, 240, 240])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**U-Net Architecture**:\n",
        "\n",
        "- **DoubleConv block**: Two consecutive convolution layers with ReLU activation capture local features efficiently.  \n",
        "- **Downsampling path**: `DoubleConv + MaxPool` layers reduce spatial dimensions while increasing feature depth, learning contextual information.  \n",
        "- **Bottleneck**: The deepest layer with the highest feature dimension, capturing the most abstract representation of the input.  \n",
        "- **Upsampling path**: Transposed convolutions increase spatial resolution, and skip connections concatenate features from the corresponding downsampling layers to recover fine details.  \n",
        "- **Output layer**: A 1Ã—1 convolution maps features to `out_ch` channels (3 tumor classes).\n",
        "\n",
        "**Input / Output**:\n",
        "\n",
        "- **Input**: (C=4, H=240, W=240) MRI slice with four modalities (T1, T1Gd, T2, T2-FLAIR).  \n",
        "- **Output**: (C=3, H=240, W=240) multi-class segmentation mask, one channel per tumor sub-region.\n",
        "\n",
        "This design allows the model to predict spatially precise tumor regions while leveraging both global context and local details.\n"
      ],
      "metadata": {
        "id": "Tu_9HsviX3uT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4c. Specify loss function for multi-class segmentation.**\n"
      ],
      "metadata": {
        "id": "YfSvAVCWcma7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Computes Dice Loss for multi-channel segmentation.\n",
        "    Handles multi-label targets with one channel per class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, smooth=1e-6):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        # Predictions and Targets: (B, C, H, W)\n",
        "        preds = torch.sigmoid(preds)  # Ensure outputs in [0,1]\n",
        "        intersection = (preds * targets).sum(dim=(2,3))\n",
        "        union = preds.sum(dim=(2,3)) + targets.sum(dim=(2,3))\n",
        "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
        "        return 1 - dice.mean()  # 1 - mean Dice across batch and channels"
      ],
      "metadata": {
        "id": "79d9NdE0oB_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate loss function\n",
        "criterion = DiceLoss()\n",
        "\n",
        "# Inspect with sample tensors (will be consolidated later as unit test modules)\n",
        "ex_preds = torch.randn(2, 3, 240, 240).to(device)\n",
        "ex_targets = torch.randint(0, 2, (2, 3, 240, 240)).float().to(device)\n",
        "loss = criterion(ex_preds, ex_targets)\n",
        "print('Sample Dice loss: ', loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCdKeZOgoCBN",
        "outputId": "88293190-329a-4394-94df-ed603424d123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Dice loss:  0.4992520809173584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Function for Multi-Class Segmentation**:\n",
        "\n",
        "Brain tumor segmentation exhibits extreme class imbalance, with most pixels belonging to healthy tissue / background. Dice Loss is well-suited for this scenario, as it directly measures the overlap between predicted and ground-truth masks.\n",
        "\n",
        "- **Multi-channel Dice Loss**: Computed independently per tumor sub-region, then averaged.  \n",
        "- **Sigmoid activation**: Converts raw logits to probabilities in [0,1] for each channel.  \n",
        "- **Smooth term**: Prevents division by zero when a class is absent in a slice.\n",
        "\n",
        "This ensures the model learns to capture small, clinically relevant tumor regions rather than being dominated by background pixels. Combining Dice Loss with standard binary cross-entropy can also stabilize training for future versions."
      ],
      "metadata": {
        "id": "RvIj41S-okTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4d. Configure optimizer and learning rate scheduler.**\n"
      ],
      "metadata": {
        "id": "Mivvj_wGcmeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure Adam optimizer (adaptive learning rates, fast convergence)\n",
        "learning_rate = 1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "L6UmWgVEqiEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up LR scheduler to reduce LR if validation Dice plateaus (help convergences)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='max',           # maximize validation Dice\n",
        "    factor=0.5,           # reduce LR by half\n",
        "    patience=3,           # wait 3 epochs without improvement\n",
        ")"
      ],
      "metadata": {
        "id": "1b2zwCHmqiKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Optimizer parameters count: ', sum(p.numel() for p in model.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLdlU0cBrY7n",
        "outputId": "5abd8cf9-d43c-41d7-ea5b-b69b04359806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer parameters count:  488462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizer and Learning Rate Scheduler**:\n",
        "\n",
        "- **Optimizer**: Adam is chosen for its adaptive learning rate capabilities and fast convergence, which is suitable for small baseline datasets.  \n",
        "- **Learning rate**: Initialized at 1e-3, balancing stability and speed of convergence.  \n",
        "- **Scheduler**: `ReduceLROnPlateau` monitors validation Dice; if no improvement occurs for 3 epochs, the learning rate is halved to encourage further optimization.\n",
        "\n",
        "This setup provides a flexible and efficient optimization loop for training the baseline U-Net without manual LR tuning."
      ],
      "metadata": {
        "id": "sS8pZn0ersO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4e. Define validation evaluation metrics.**\n"
      ],
      "metadata": {
        "id": "-VjdEKFxvAuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_dice(preds, targets, threshold=0.5, smooth=1e-6):\n",
        "    \"\"\"\n",
        "    Compute per-channel and mean Dice coefficient for multi-label masks.\n",
        "\n",
        "    Args:\n",
        "        preds (torch.Tensor): Model outputs (B, C, H, W) raw logits.\n",
        "        targets (torch.Tensor): Ground-truth masks (B, C, H, W) in {0,1}.\n",
        "        threshold (float): Sigmoid threshold to binarize predictions.\n",
        "        smooth (float): Small value to avoid division by zero.\n",
        "\n",
        "    Returns:\n",
        "        dice_per_channel (list of float): Dice for each mask channel.\n",
        "        mean_dice (float): Average Dice across channels.\n",
        "    \"\"\"\n",
        "\n",
        "    # Apply sigmoid and binarize\n",
        "    preds = torch.sigmoid(preds) > threshold\n",
        "\n",
        "    # Flatten spatial dimensions\n",
        "    preds_flat = preds.view(preds.size(0), preds.size(1), -1).float()\n",
        "    targets_flat = targets.view(targets.size(0), targets.size(1), -1).float()\n",
        "\n",
        "    # Compute intersection and union per channel\n",
        "    intersection = (preds_flat * targets_flat).sum(dim=2)\n",
        "    union = preds_flat.sum(dim=2) + targets_flat.sum(dim=2)\n",
        "\n",
        "    # Dice per channel for each sample\n",
        "    dice = (2 * intersection + smooth) / (union + smooth)\n",
        "\n",
        "    # Average over batch\n",
        "    dice_per_channel = dice.mean(dim=0).cpu().tolist()\n",
        "    mean_dice = dice.mean().item()\n",
        "\n",
        "    return dice_per_channel, mean_dice"
      ],
      "metadata": {
        "id": "w-W6m7k-vGkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation Dice Evaluation**:\n",
        "\n",
        "This function computes Dice scores for multi-label segmentation:\n",
        "\n",
        "- **Per-channel Dice**: Calculates Dice for each tumor sub-region independently.  \n",
        "- **Mean Dice**: Average across all channels to summarize overall performance.  \n",
        "- **Binarization**: Predictions are converted from raw logits to 0/1 using a sigmoid threshold (default 0.5).  \n",
        "- **Batch-wise averaging**: Dice is first computed per slice, then averaged over the batch.\n",
        "\n",
        "This helper allows efficient evaluation of model performance on the validation set at the end of each epoch or after training without adding significant compute overhead."
      ],
      "metadata": {
        "id": "bssQsIFfvMG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_precision_recall(preds, targets, eps=1e-6):\n",
        "    \"\"\"\n",
        "    Compute per-channel precision and recall.\n",
        "    preds: torch.Tensor (B, C, H, W), binary\n",
        "    targets: torch.Tensor (B, C, H, W), binary\n",
        "    eps: small value to avoid division by zero\n",
        "    \"\"\"\n",
        "    C = targets.shape[1]\n",
        "    precision = []\n",
        "    recall = []\n",
        "\n",
        "    for c in range(C):\n",
        "        pred_c = preds[:, c].flatten()\n",
        "        target_c = targets[:, c].flatten()\n",
        "\n",
        "        TP = (pred_c * target_c).sum().float()\n",
        "        FP = ((pred_c == 1) & (target_c == 0)).sum().float()\n",
        "        FN = ((pred_c == 0) & (target_c == 1)).sum().float()\n",
        "\n",
        "        precision.append((TP / (TP + FP + eps)).item())\n",
        "        recall.append((TP / (TP + FN + eps)).item())\n",
        "\n",
        "    return precision, recall"
      ],
      "metadata": {
        "id": "Vq59wxC7a4RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4f. Set up remaining training parameters.**\n"
      ],
      "metadata": {
        "id": "2AtZU4RpcmhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "num_epochs = 8\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Move model to device (GPU or CPU)\n",
        "model.to(device)\n",
        "\n",
        "print(f'Training is set up to run on {device} for {num_epochs} epochs with batch size {batch_size}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WeofiZNsOUi",
        "outputId": "02a69909-9103-424f-a049-2e5035d6a49e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training is set up to run on cuda for 8 epochs with batch size 8.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The baseline U-Net, loss function, optimizer, scheduler, and training parameters are now fully defined. The model is moved to GPU if available for faster computation. **DiceLoss** handles each mask channel as an independent binary label, making it ideal for multi-label segmentation. The **Adam optimizer** provides adaptive learning rates for efficient convergence. `num_epochs` sets how many times the dataset is seen during training, and `batch_size` controls memory usage and gradient stability. With all components in place, the model is ready for training in the next section."
      ],
      "metadata": {
        "id": "89huX2KWbxty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 5. Model Training ðŸ¤–\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "aKEolpmAd0B3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up path for checkpoint saves\n",
        "drive_models_path = '/content/drive/MyDrive/modelsv2'\n",
        "os.makedirs(drive_models_path, exist_ok=True)\n",
        "\n",
        "print('Checkpoints will be saved to: ', drive_models_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0Gk2m5QxU21",
        "outputId": "f124a4e2-0082-4e8b-ec2a-8d0394bca4a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoints will be saved to:  /content/drive/MyDrive/modelsv2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate an empty dataframe to capture metrics during training\n",
        "metrics_df = pd.DataFrame(columns=[\n",
        "    'epoch', 'train_loss', 'val_loss', 'mean_dice',\n",
        "    'dice_NEC/NET', 'dice_ED', 'dice_ET',\n",
        "    'prec_NEC/NET', 'prec_ED', 'prec_ET',\n",
        "    'recall_NEC/NET', 'recall_ED', 'recall_ET'\n",
        "])"
      ],
      "metadata": {
        "id": "z1AGyaUxbSYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, num_epochs + 1 + 7):\n",
        "\n",
        "    # -------------------------------\n",
        "    # Training Phase\n",
        "    # -------------------------------\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for images, masks in tqdm(train_loader, desc=f'Epoch {epoch} [Train]'):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()             # Reset gradients\n",
        "        outputs = model(images)           # Forward pass\n",
        "        loss = criterion(outputs, masks)  # Dice loss\n",
        "        loss.backward()                   # Backpropagate\n",
        "        optimizer.step()                  # Update weights\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)  # Average over all slices\n",
        "\n",
        "    # -------------------------------\n",
        "    # Checkpoint Save\n",
        "    # -------------------------------\n",
        "    checkpoint_path = os.path.join(drive_models_path, f'v1_epoch_{epoch}.pt')\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': train_loss,\n",
        "    }, checkpoint_path)\n",
        "    print(f'Checkpoint saved: {checkpoint_path}')\n",
        "\n",
        "    # -------------------------------\n",
        "    # Validation Phase\n",
        "    # -------------------------------\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    dice_accum = []\n",
        "    precision_accum = []\n",
        "    recall_accum = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(val_loader, desc=f'Epoch {epoch} [Val]'):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            preds = (probs > 0.5).float()\n",
        "\n",
        "            # Compute Dice per channel\n",
        "            dice_ch, mean_dice = compute_dice(outputs, masks)\n",
        "            dice_accum.append(dice_ch)\n",
        "\n",
        "            # Compute precision & recall per channel\n",
        "            prec, rec = compute_precision_recall(preds, masks)\n",
        "            precision_accum.append(prec)\n",
        "            recall_accum.append(rec)\n",
        "\n",
        "            # Accumulate validation loss\n",
        "            val_loss += criterion(outputs, masks).item() * images.size(0)\n",
        "\n",
        "    # Average metrics across batches\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    dice_avg = [sum(col)/len(col) for col in zip(*dice_accum)]\n",
        "    mean_dice_avg = sum(dice_avg)/len(dice_avg)\n",
        "    precision_avg = [sum(col)/len(col) for col in zip(*precision_accum)]\n",
        "    recall_avg = [sum(col)/len(col) for col in zip(*recall_accum)]\n",
        "\n",
        "    # -------------------------------\n",
        "    # Scheduler Step\n",
        "    # -------------------------------\n",
        "    scheduler.step(mean_dice_avg)  # Adjust LR based on validation Dice\n",
        "\n",
        "    # -------------------------------\n",
        "    # Epoch Summary\n",
        "    # -------------------------------\n",
        "    print(f'Epoch: {epoch} | '\n",
        "          f'Train Loss: {train_loss:.4f} | '\n",
        "          f'Val Loss: {val_loss:.4f} | '\n",
        "          f'Mean Dice: {mean_dice_avg:.4f} | '\n",
        "          f'Dice per channel: {dice_avg} | '\n",
        "          f'Precision per channel: {precision_avg} | '\n",
        "          f'Recall per channel: {recall_avg}')\n",
        "\n",
        "    # -------------------------------\n",
        "    # Log metrics into DataFrame\n",
        "    # -------------------------------\n",
        "    metrics_df.loc[len(metrics_df)] = [\n",
        "        epoch, train_loss, val_loss, mean_dice_avg,\n",
        "        dice_avg[0], dice_avg[1], dice_avg[2],\n",
        "        precision_avg[0], precision_avg[1], precision_avg[2],\n",
        "        recall_avg[0], recall_avg[1], recall_avg[2]\n",
        "    ]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiTCiVpxvxp7",
        "outputId": "c4548e83-39c4-42cd-95ab-7f86d1837305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:15<00:00,  9.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved: /content/drive/MyDrive/modelsv2/v1_epoch_1.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:03<00:00, 10.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | Train Loss: 0.7639 | Val Loss: 0.8382 | Mean Dice: 0.3904 | Dice per channel: [0.32651993645607524, 0.38355322176873513, 0.46100796951308537] | Precision per channel: [0.03892471035420059, 0.3526815160473164, 0.20767884976111162] | Recall per channel: [0.23262617144829187, 0.35697237726969594, 0.21754443483092847]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:16<00:00,  9.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved: /content/drive/MyDrive/modelsv2/v1_epoch_2.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:03<00:00, 10.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 | Train Loss: 0.7647 | Val Loss: 0.8362 | Mean Dice: 0.3828 | Dice per channel: [0.31345343411958826, 0.3860113890000177, 0.4489995077036494] | Precision per channel: [0.045267075806175575, 0.35136369465498063, 0.20708317474390453] | Recall per channel: [0.22946008333028892, 0.36570191517090184, 0.2193448304747924]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:15<00:00,  9.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved: /content/drive/MyDrive/modelsv2/v1_epoch_3.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:03<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 | Train Loss: 0.7630 | Val Loss: 0.8395 | Mean Dice: 0.3624 | Dice per channel: [0.26288567913231387, 0.37948631619276346, 0.444938583607661] | Precision per channel: [0.025113694358599156, 0.34623947098346736, 0.20469746582257825] | Recall per channel: [0.23522310379223946, 0.35232930124188083, 0.21430771454022482]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:15<00:00,  9.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved: /content/drive/MyDrive/modelsv2/v1_epoch_4.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:04<00:00,  8.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 | Train Loss: 0.7658 | Val Loss: 0.8358 | Mean Dice: 0.3920 | Dice per channel: [0.33074962465610225, 0.38397256034653243, 0.46132176032382577] | Precision per channel: [0.04791971160478006, 0.35125105615812713, 0.20892229474460086] | Recall per channel: [0.22938528332190636, 0.36178302354155445, 0.21289823204278946]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:16<00:00,  9.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved: /content/drive/MyDrive/modelsv2/v1_epoch_5.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:03<00:00, 12.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5 | Train Loss: 0.7654 | Val Loss: 0.8323 | Mean Dice: 0.3836 | Dice per channel: [0.30796643921910993, 0.391197000806792, 0.4517573361450863] | Precision per channel: [0.047748569339418255, 0.3542507890558157, 0.2038866338582757] | Recall per channel: [0.23239038502558684, 0.37551711633419377, 0.2232894574602445]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:15<00:00,  9.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved: /content/drive/MyDrive/modelsv2/v1_epoch_6.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:03<00:00, 12.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6 | Train Loss: 0.7654 | Val Loss: 0.8374 | Mean Dice: 0.3551 | Dice per channel: [0.34013127009774585, 0.25501153217570516, 0.4702821765740293] | Precision per channel: [0.046483594549592964, 0.3544446955530498, 0.2092193304680479] | Recall per channel: [0.2329924408441935, 0.3537273617127003, 0.21632384174527267]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:15<00:00,  9.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved: /content/drive/MyDrive/modelsv2/v1_epoch_7.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:04<00:00,  8.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7 | Train Loss: 0.7655 | Val Loss: 0.8359 | Mean Dice: 0.3623 | Dice per channel: [0.34496160883864047, 0.25602251570875206, 0.48602313787504303] | Precision per channel: [0.04892375900631006, 0.34983952404955065, 0.2089935363485263] | Recall per channel: [0.23509994454872915, 0.35460634777943295, 0.2180615738989451]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:15<00:00,  9.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved: /content/drive/MyDrive/modelsv2/v1_epoch_8.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:03<00:00, 12.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8 | Train Loss: 0.7644 | Val Loss: 0.8385 | Mean Dice: 0.3847 | Dice per channel: [0.3184618269766145, 0.38143385000436725, 0.4543324712861656] | Precision per channel: [0.02605449909923407, 0.3484660892677195, 0.20908504098844835] | Recall per channel: [0.2319609060501441, 0.35816910232488924, 0.20947993479860136]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:15<00:00,  9.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved: /content/drive/MyDrive/modelsv2/v1_epoch_9.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:03<00:00, 12.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9 | Train Loss: 0.7633 | Val Loss: 0.8358 | Mean Dice: 0.3914 | Dice per channel: [0.3336492096366422, 0.38521757439498255, 0.45540495911334544] | Precision per channel: [0.04806680845257898, 0.35169740857478854, 0.20693118278032693] | Recall per channel: [0.22738512414388168, 0.3637900940882854, 0.21990563529424179]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:15<00:00,  9.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved: /content/drive/MyDrive/modelsv2/v1_epoch_10.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:04<00:00,  8.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 | Train Loss: 0.7627 | Val Loss: 0.8355 | Mean Dice: 0.3967 | Dice per channel: [0.3427869458020974, 0.3855821890568942, 0.4616305637314924] | Precision per channel: [0.047191627929584146, 0.35321369900768146, 0.20528871200692195] | Recall per channel: [0.2318720592138095, 0.3621670248416754, 0.22237385522860748]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:15<00:00,  9.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved: /content/drive/MyDrive/modelsv2/v1_epoch_11.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:03<00:00, 12.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11 | Train Loss: 0.7648 | Val Loss: 0.8386 | Mean Dice: 0.3720 | Dice per channel: [0.2895481295953363, 0.3814185928916518, 0.44504888753786714] | Precision per channel: [0.016621347830200996, 0.3470690237591043, 0.20915047890052962] | Recall per channel: [0.22796763498813677, 0.35856019781950194, 0.21007564425086364]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:15<00:00,  9.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved: /content/drive/MyDrive/modelsv2/v1_epoch_12.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:03<00:00, 12.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 12 | Train Loss: 0.7641 | Val Loss: 0.8366 | Mean Dice: 0.4010 | Dice per channel: [0.34570948605273333, 0.3805178875129766, 0.4767842777312012] | Precision per channel: [0.04655867798749405, 0.3536036938416939, 0.20780588244684997] | Recall per channel: [0.23685628366776001, 0.35494719235560834, 0.2185760170030288]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:15<00:00,  9.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved: /content/drive/MyDrive/modelsv2/v1_epoch_13.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:04<00:00,  8.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 13 | Train Loss: 0.7640 | Val Loss: 0.8373 | Mean Dice: 0.3832 | Dice per channel: [0.30118893318340234, 0.3815457748820377, 0.46698166473278974] | Precision per channel: [0.04706144513329491, 0.35147509205256566, 0.2082222283531267] | Recall per channel: [0.24116778411926368, 0.352524197445466, 0.21208561374208865]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:15<00:00,  9.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved: /content/drive/MyDrive/modelsv2/v1_epoch_14.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:03<00:00, 12.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 14 | Train Loss: 0.7646 | Val Loss: 0.8334 | Mean Dice: 0.4083 | Dice per channel: [0.3502999099347702, 0.38769709740914143, 0.48678222202961363] | Precision per channel: [0.04761323568303711, 0.35815320962156433, 0.21124038372475368] | Recall per channel: [0.23802832953440836, 0.3632135608066351, 0.21069659474186408]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:15<00:00,  9.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved: /content/drive/MyDrive/modelsv2/v1_epoch_15.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:03<00:00, 12.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 15 | Train Loss: 0.7643 | Val Loss: 0.8375 | Mean Dice: 0.3939 | Dice per channel: [0.33596533450086913, 0.3815589343166989, 0.4641946430659382] | Precision per channel: [0.046392678518970616, 0.35525539331585887, 0.20523523883177683] | Recall per channel: [0.23856897728565413, 0.35082337260246277, 0.2206860358516375]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This loop iterates over epochs, updating the model on training slices and evaluating its performance on the validation set:\n",
        "\n",
        "- **Training phase**: Computes Dice loss, backpropagates, and updates weights via Adam optimizer.  \n",
        "- **Validation phase**: Disables gradients and computes both loss and Dice metrics for each tumor sub-region using the evaluation helper function from the previous section.  \n",
        "- **Per-channel Dice**: Monitors performance on necrotic core, edema, and enhancing tumor independently.  \n",
        "- **Mean Dice**: Average of all channels provides a single performance summary.  \n",
        "- **Scheduler step**: Optionally reduces learning rate if validation Dice plateaus.  \n",
        "- **Epoch summary**: Prints training loss, validation loss, mean Dice, and per-channel Dice for monitoring progress.\n",
        "\n",
        "This setup ensures the baseline model is trained efficiently while keeping track of clinically relevant metrics despite extreme class imbalance."
      ],
      "metadata": {
        "id": "Bfh3l8RzvyBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 6. Model Evaluation ðŸ“‹\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "duUvpkI58M_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "sUFeM0BKe9iI",
        "outputId": "974e2804-3fc6-4f9c-fdcd-52fc2563c7b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    epoch  train_loss  val_loss  mean_dice  dice_NEC/NET   dice_ED   dice_ET  \\\n",
              "0     1.0    0.763911  0.838182   0.390360      0.326520  0.383553  0.461008   \n",
              "1     2.0    0.764681  0.836168   0.382821      0.313453  0.386011  0.449000   \n",
              "2     3.0    0.763022  0.839544   0.362437      0.262886  0.379486  0.444939   \n",
              "3     4.0    0.765850  0.835812   0.392015      0.330750  0.383973  0.461322   \n",
              "4     5.0    0.765439  0.832251   0.383640      0.307966  0.391197  0.451757   \n",
              "5     6.0    0.765421  0.837389   0.355142      0.340131  0.255012  0.470282   \n",
              "6     7.0    0.765534  0.835873   0.362336      0.344962  0.256023  0.486023   \n",
              "7     8.0    0.764397  0.838480   0.384743      0.318462  0.381434  0.454332   \n",
              "8     9.0    0.763341  0.835816   0.391424      0.333649  0.385218  0.455405   \n",
              "9    10.0    0.762712  0.835529   0.396667      0.342787  0.385582  0.461631   \n",
              "10   11.0    0.764775  0.838609   0.372005      0.289548  0.381419  0.445049   \n",
              "11   12.0    0.764104  0.836586   0.401004      0.345709  0.380518  0.476784   \n",
              "12   13.0    0.764007  0.837264   0.383239      0.301189  0.381546  0.466982   \n",
              "13   14.0    0.764609  0.833403   0.408260      0.350300  0.387697  0.486782   \n",
              "14   15.0    0.764260  0.837463   0.393906      0.335965  0.381559  0.464195   \n",
              "\n",
              "    prec_NEC/NET   prec_ED   prec_ET  recall_NEC/NET  recall_ED  recall_ET  \n",
              "0       0.038925  0.352682  0.207679        0.232626   0.356972   0.217544  \n",
              "1       0.045267  0.351364  0.207083        0.229460   0.365702   0.219345  \n",
              "2       0.025114  0.346239  0.204697        0.235223   0.352329   0.214308  \n",
              "3       0.047920  0.351251  0.208922        0.229385   0.361783   0.212898  \n",
              "4       0.047749  0.354251  0.203887        0.232390   0.375517   0.223289  \n",
              "5       0.046484  0.354445  0.209219        0.232992   0.353727   0.216324  \n",
              "6       0.048924  0.349840  0.208994        0.235100   0.354606   0.218062  \n",
              "7       0.026054  0.348466  0.209085        0.231961   0.358169   0.209480  \n",
              "8       0.048067  0.351697  0.206931        0.227385   0.363790   0.219906  \n",
              "9       0.047192  0.353214  0.205289        0.231872   0.362167   0.222374  \n",
              "10      0.016621  0.347069  0.209150        0.227968   0.358560   0.210076  \n",
              "11      0.046559  0.353604  0.207806        0.236856   0.354947   0.218576  \n",
              "12      0.047061  0.351475  0.208222        0.241168   0.352524   0.212086  \n",
              "13      0.047613  0.358153  0.211240        0.238028   0.363214   0.210697  \n",
              "14      0.046393  0.355255  0.205235        0.238569   0.350823   0.220686  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aac33aa2-8153-4b15-acd5-e23211f21328\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>mean_dice</th>\n",
              "      <th>dice_NEC/NET</th>\n",
              "      <th>dice_ED</th>\n",
              "      <th>dice_ET</th>\n",
              "      <th>prec_NEC/NET</th>\n",
              "      <th>prec_ED</th>\n",
              "      <th>prec_ET</th>\n",
              "      <th>recall_NEC/NET</th>\n",
              "      <th>recall_ED</th>\n",
              "      <th>recall_ET</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.763911</td>\n",
              "      <td>0.838182</td>\n",
              "      <td>0.390360</td>\n",
              "      <td>0.326520</td>\n",
              "      <td>0.383553</td>\n",
              "      <td>0.461008</td>\n",
              "      <td>0.038925</td>\n",
              "      <td>0.352682</td>\n",
              "      <td>0.207679</td>\n",
              "      <td>0.232626</td>\n",
              "      <td>0.356972</td>\n",
              "      <td>0.217544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.764681</td>\n",
              "      <td>0.836168</td>\n",
              "      <td>0.382821</td>\n",
              "      <td>0.313453</td>\n",
              "      <td>0.386011</td>\n",
              "      <td>0.449000</td>\n",
              "      <td>0.045267</td>\n",
              "      <td>0.351364</td>\n",
              "      <td>0.207083</td>\n",
              "      <td>0.229460</td>\n",
              "      <td>0.365702</td>\n",
              "      <td>0.219345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>0.763022</td>\n",
              "      <td>0.839544</td>\n",
              "      <td>0.362437</td>\n",
              "      <td>0.262886</td>\n",
              "      <td>0.379486</td>\n",
              "      <td>0.444939</td>\n",
              "      <td>0.025114</td>\n",
              "      <td>0.346239</td>\n",
              "      <td>0.204697</td>\n",
              "      <td>0.235223</td>\n",
              "      <td>0.352329</td>\n",
              "      <td>0.214308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.0</td>\n",
              "      <td>0.765850</td>\n",
              "      <td>0.835812</td>\n",
              "      <td>0.392015</td>\n",
              "      <td>0.330750</td>\n",
              "      <td>0.383973</td>\n",
              "      <td>0.461322</td>\n",
              "      <td>0.047920</td>\n",
              "      <td>0.351251</td>\n",
              "      <td>0.208922</td>\n",
              "      <td>0.229385</td>\n",
              "      <td>0.361783</td>\n",
              "      <td>0.212898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>0.765439</td>\n",
              "      <td>0.832251</td>\n",
              "      <td>0.383640</td>\n",
              "      <td>0.307966</td>\n",
              "      <td>0.391197</td>\n",
              "      <td>0.451757</td>\n",
              "      <td>0.047749</td>\n",
              "      <td>0.354251</td>\n",
              "      <td>0.203887</td>\n",
              "      <td>0.232390</td>\n",
              "      <td>0.375517</td>\n",
              "      <td>0.223289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6.0</td>\n",
              "      <td>0.765421</td>\n",
              "      <td>0.837389</td>\n",
              "      <td>0.355142</td>\n",
              "      <td>0.340131</td>\n",
              "      <td>0.255012</td>\n",
              "      <td>0.470282</td>\n",
              "      <td>0.046484</td>\n",
              "      <td>0.354445</td>\n",
              "      <td>0.209219</td>\n",
              "      <td>0.232992</td>\n",
              "      <td>0.353727</td>\n",
              "      <td>0.216324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7.0</td>\n",
              "      <td>0.765534</td>\n",
              "      <td>0.835873</td>\n",
              "      <td>0.362336</td>\n",
              "      <td>0.344962</td>\n",
              "      <td>0.256023</td>\n",
              "      <td>0.486023</td>\n",
              "      <td>0.048924</td>\n",
              "      <td>0.349840</td>\n",
              "      <td>0.208994</td>\n",
              "      <td>0.235100</td>\n",
              "      <td>0.354606</td>\n",
              "      <td>0.218062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8.0</td>\n",
              "      <td>0.764397</td>\n",
              "      <td>0.838480</td>\n",
              "      <td>0.384743</td>\n",
              "      <td>0.318462</td>\n",
              "      <td>0.381434</td>\n",
              "      <td>0.454332</td>\n",
              "      <td>0.026054</td>\n",
              "      <td>0.348466</td>\n",
              "      <td>0.209085</td>\n",
              "      <td>0.231961</td>\n",
              "      <td>0.358169</td>\n",
              "      <td>0.209480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9.0</td>\n",
              "      <td>0.763341</td>\n",
              "      <td>0.835816</td>\n",
              "      <td>0.391424</td>\n",
              "      <td>0.333649</td>\n",
              "      <td>0.385218</td>\n",
              "      <td>0.455405</td>\n",
              "      <td>0.048067</td>\n",
              "      <td>0.351697</td>\n",
              "      <td>0.206931</td>\n",
              "      <td>0.227385</td>\n",
              "      <td>0.363790</td>\n",
              "      <td>0.219906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10.0</td>\n",
              "      <td>0.762712</td>\n",
              "      <td>0.835529</td>\n",
              "      <td>0.396667</td>\n",
              "      <td>0.342787</td>\n",
              "      <td>0.385582</td>\n",
              "      <td>0.461631</td>\n",
              "      <td>0.047192</td>\n",
              "      <td>0.353214</td>\n",
              "      <td>0.205289</td>\n",
              "      <td>0.231872</td>\n",
              "      <td>0.362167</td>\n",
              "      <td>0.222374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11.0</td>\n",
              "      <td>0.764775</td>\n",
              "      <td>0.838609</td>\n",
              "      <td>0.372005</td>\n",
              "      <td>0.289548</td>\n",
              "      <td>0.381419</td>\n",
              "      <td>0.445049</td>\n",
              "      <td>0.016621</td>\n",
              "      <td>0.347069</td>\n",
              "      <td>0.209150</td>\n",
              "      <td>0.227968</td>\n",
              "      <td>0.358560</td>\n",
              "      <td>0.210076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12.0</td>\n",
              "      <td>0.764104</td>\n",
              "      <td>0.836586</td>\n",
              "      <td>0.401004</td>\n",
              "      <td>0.345709</td>\n",
              "      <td>0.380518</td>\n",
              "      <td>0.476784</td>\n",
              "      <td>0.046559</td>\n",
              "      <td>0.353604</td>\n",
              "      <td>0.207806</td>\n",
              "      <td>0.236856</td>\n",
              "      <td>0.354947</td>\n",
              "      <td>0.218576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13.0</td>\n",
              "      <td>0.764007</td>\n",
              "      <td>0.837264</td>\n",
              "      <td>0.383239</td>\n",
              "      <td>0.301189</td>\n",
              "      <td>0.381546</td>\n",
              "      <td>0.466982</td>\n",
              "      <td>0.047061</td>\n",
              "      <td>0.351475</td>\n",
              "      <td>0.208222</td>\n",
              "      <td>0.241168</td>\n",
              "      <td>0.352524</td>\n",
              "      <td>0.212086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14.0</td>\n",
              "      <td>0.764609</td>\n",
              "      <td>0.833403</td>\n",
              "      <td>0.408260</td>\n",
              "      <td>0.350300</td>\n",
              "      <td>0.387697</td>\n",
              "      <td>0.486782</td>\n",
              "      <td>0.047613</td>\n",
              "      <td>0.358153</td>\n",
              "      <td>0.211240</td>\n",
              "      <td>0.238028</td>\n",
              "      <td>0.363214</td>\n",
              "      <td>0.210697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15.0</td>\n",
              "      <td>0.764260</td>\n",
              "      <td>0.837463</td>\n",
              "      <td>0.393906</td>\n",
              "      <td>0.335965</td>\n",
              "      <td>0.381559</td>\n",
              "      <td>0.464195</td>\n",
              "      <td>0.046393</td>\n",
              "      <td>0.355255</td>\n",
              "      <td>0.205235</td>\n",
              "      <td>0.238569</td>\n",
              "      <td>0.350823</td>\n",
              "      <td>0.220686</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aac33aa2-8153-4b15-acd5-e23211f21328')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-aac33aa2-8153-4b15-acd5-e23211f21328 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-aac33aa2-8153-4b15-acd5-e23211f21328');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_06893332-b503-4a93-9e70-9821628fb542\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('metrics_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_06893332-b503-4a93-9e70-9821628fb542 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('metrics_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "metrics_df",
              "summary": "{\n  \"name\": \"metrics_df\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"epoch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.47213595499958,\n        \"min\": 1.0,\n        \"max\": 15.0,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          10.0,\n          12.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0009313575549021788,\n        \"min\": 0.7627121579262518,\n        \"max\": 0.7658496960516898,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.7627121579262518,\n          0.7641043459215472,\n          0.7639111595769082\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0019412685852377455,\n        \"min\": 0.8322508685050473,\n        \"max\": 0.8395443216446907,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.835529064363049,\n          0.8365861285117364,\n          0.8381817025523032\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_dice\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.015141438966168961,\n        \"min\": 0.3551416596158268,\n        \"max\": 0.40825974312450847,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.396666566196828,\n          0.40100388376563706,\n          0.39036037591263195\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dice_NEC/NET\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.024390368258821706,\n        \"min\": 0.26288567913231387,\n        \"max\": 0.3502999099347702,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.3427869458020974,\n          0.34570948605273333,\n          0.32651993645607524\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dice_ED\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04523657456614382,\n        \"min\": 0.25501153217570516,\n        \"max\": 0.391197000806792,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.3855821890568942,\n          0.3805178875129766,\n          0.38355322176873513\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dice_ET\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.013261884875123255,\n        \"min\": 0.444938583607661,\n        \"max\": 0.48678222202961363,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.4616305637314924,\n          0.4767842777312012,\n          0.46100796951308537\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prec_NEC/NET\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.01034963227307409,\n        \"min\": 0.016621347830200996,\n        \"max\": 0.04892375900631006,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.047191627929584146,\n          0.04655867798749405,\n          0.03892471035420059\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prec_ED\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0031615178558554234,\n        \"min\": 0.34623947098346736,\n        \"max\": 0.35815320962156433,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.35321369900768146,\n          0.3536036938416939,\n          0.3526815160473164\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prec_ET\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0020433101151710073,\n        \"min\": 0.2038866338582757,\n        \"max\": 0.21124038372475368,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.20528871200692195,\n          0.20780588244684997,\n          0.20767884976111162\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall_NEC/NET\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004054606522879621,\n        \"min\": 0.22738512414388168,\n        \"max\": 0.24116778411926368,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.2318720592138095,\n          0.23685628366776001,\n          0.23262617144829187\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall_ED\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.006535725859586408,\n        \"min\": 0.35082337260246277,\n        \"max\": 0.37551711633419377,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.3621670248416754,\n          0.35494719235560834,\n          0.35697237726969594\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall_ET\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004527329469150792,\n        \"min\": 0.20947993479860136,\n        \"max\": 0.2232894574602445,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.22237385522860748,\n          0.2185760170030288,\n          0.21754443483092847\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Validation Metrics**:\n",
        "\n",
        "After 15 epochs of training, the model is evaluated on the held-out validation patient. The final scores are displayed above. Based on this, the performance did not improve over the baseline. Therefore, future versions should continue configuring the model architecture for better results."
      ],
      "metadata": {
        "id": "W243oXUl8qUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "channel_colors = ['maroon', 'yellowgreen', 'red']\n",
        "channel_names  = ['NEC/NET', 'ED', 'ET']\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "axes = axes.flatten()  # easier indexing\n",
        "\n",
        "# Training & Validation Loss\n",
        "axes[0].plot(metrics_df['epoch'], metrics_df['train_loss'], label='Train Loss', color='blue', marker='o')\n",
        "axes[0].plot(metrics_df['epoch'], metrics_df['val_loss'], label='Val Loss', color='orange', marker='o')\n",
        "axes[0].set_title('Training & Validation Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Mean Dice\n",
        "axes[1].plot(metrics_df['epoch'], metrics_df['mean_dice'], label='Mean Dice', color='green', marker='o')\n",
        "axes[1].set_title('Validation Mean Dice')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Dice Score')\n",
        "axes[1].set_ylim(0, 1)\n",
        "axes[1].grid(True)\n",
        "\n",
        "# Dice per channel\n",
        "for i, ch in enumerate(channel_names):\n",
        "    axes[2].plot(metrics_df['epoch'], metrics_df[f'dice_{ch}'], label=f'Dice {ch}', color=channel_colors[i], marker='o')\n",
        "axes[2].set_title('Dice per Tumor Channel')\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].set_ylabel('Dice Score')\n",
        "axes[2].set_ylim(0, 1)\n",
        "axes[2].legend()\n",
        "axes[2].grid(True)\n",
        "\n",
        "# Precision & Recall per channel\n",
        "for i, ch in enumerate(channel_names):\n",
        "    axes[3].plot(metrics_df['epoch'], metrics_df[f'prec_{ch}'], label=f'Precision {ch}', color=channel_colors[i], marker='o', linestyle='-')\n",
        "    axes[3].plot(metrics_df['epoch'], metrics_df[f'recall_{ch}'], label=f'Recall {ch}', color=channel_colors[i], marker='x', linestyle='--')\n",
        "axes[3].set_title('Precision & Recall per Tumor Channel')\n",
        "axes[3].set_xlabel('Epoch')\n",
        "axes[3].set_ylabel('Score')\n",
        "axes[3].set_ylim(0, 1)\n",
        "axes[3].legend()\n",
        "axes[3].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LQwWrE_Dfx8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a specific validation slice\n",
        "slice_index = 23\n",
        "\n",
        "image, mask = val_dataset[slice_index]\n",
        "\n",
        "# Add batch dimension (model expects B,C,H,W)\n",
        "image = image.unsqueeze(0).to(device)\n",
        "mask = mask.unsqueeze(0).to(device)\n",
        "\n",
        "# Set model to eval mode\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(image)\n",
        "    probs = torch.sigmoid(output)\n",
        "    pred = (probs > 0.5).float()\n",
        "\n",
        "# Move everything to CPU for plotting\n",
        "image = image.cpu()\n",
        "mask = mask.cpu()\n",
        "pred = pred.cpu()\n",
        "\n",
        "# Remove batch dimension for plotting\n",
        "image = image[0]\n",
        "mask = mask[0]\n",
        "pred = pred[0]\n",
        "\n",
        "# Set up figure\n",
        "fig, axes = plt.subplots(2, 3, figsize=(10, 10))\n",
        "\n",
        "# Iterable colors and plot titles\n",
        "channel_colors = ['maroon', 'yellowgreen', 'red']\n",
        "actual_titles = ['Annotated NEC/NET', 'Annotated ED', 'Annotated ET']\n",
        "pred_titles = ['Predicted NEC/NET', 'Predicted ED', 'Predicted ET']\n",
        "\n",
        "# Ground Truth\n",
        "for c in range(3):\n",
        "    annotated_cmap = ListedColormap(['black', channel_colors[c]])\n",
        "    axes[0, c].imshow(mask[c], cmap=annotated_cmap, vmin=0, vmax=1)\n",
        "    axes[0, c].set_title(actual_titles[c])\n",
        "    axes[0, c].axis('off')\n",
        "\n",
        "# Predictions\n",
        "for c in range(3):\n",
        "    pred_cmap = ListedColormap(['black', channel_colors[c]])\n",
        "    axes[1, c].imshow(pred[c], cmap=pred_cmap, vmin=0, vmax=1)\n",
        "    axes[1, c].set_title(pred_titles[c])\n",
        "    axes[1, c].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "v59Ct6Cv8byZ",
        "outputId": "69972cc8-4ebd-43ab-cdd3-8d7e9f34c7a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen from a slice example, the visual evaluation can be misleading as this particular slice shows strong predictions while the evaluation metrics are not up to par."
      ],
      "metadata": {
        "id": "U1XKoCd_gsi6"
      }
    }
  ]
}